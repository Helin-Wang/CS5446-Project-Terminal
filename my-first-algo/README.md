# My-First-Algo

## ğŸ§© Overview

This project implements a **simple reinforcement learning (RL) bot** designed to interact with a starter game environment.  
It focuses on lightweight experimentation â€” **no deep learning or environment simulation**, just local play, logging, and policy updates.

---

## ğŸš€ Stage 0 â€” Minimal Viable Bot

### ğŸ¯ Goal

Develop a bot that can:

1. **Extract Observations**  
   - Capture a compact **observation vector** each turn from the game state.

2. **Select Actions**  
   - Choose **one macro-action** from a predefined set.  
   - Apply **feasibility masking** to filter out invalid options.

3. **Execute Macros**  
   - Use the **starter GameState API** to perform high-level actions  
     (e.g., building, spawning, etc.).

4. **Log Experiences**  
   - Record **(state, action, reward)** transitions to a `.jsonl` file for later training.

5. **Train Offline**  
   - Use a simple **REINFORCE** algorithm to train a lightweight policy:  
     - Model = softmax over macro-actions  
     - No neural networks â€” just basic parameter updates  
   - After training, **reload the updated weights** into the bot.

---

## ğŸ§  Key Principles

- **Simplicity First:** Avoid deep learning; start with interpretable, small-scale logic.  
- **Offline Learning:** Gather data from games played against the starter bot, then train offline.  
- **Reproducibility:** All logs and weights can be easily reloaded for iteration.  

---

## ğŸ“ Outputs

- `trajectories.jsonl` â€” recorded (s, a, r) transitions  
- `policy_weights.json` â€” trained macro-policy parameters  

---

## ğŸ•¹ï¸ Next Steps

- Improve observation encoding  
- Expand macro-action set  
- Add baselines or entropy regularization to stabilize REINFORCE  
